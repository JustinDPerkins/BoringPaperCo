version: "3.8"

############################
#  SERVICES
############################
services:
  # ─────────── SDK API ───────────
  sdk-service:
    build:
      context: ../sdk
    ports:
      - "5000:5000"
    env_file:                
      - .env
    environment:
      API_KEY: ${API_KEY}    
      REGION:  ${REGION}
    restart: unless-stopped
    networks:
      - bpc-net

  # ───────── Container XDR ────────
  containerxdr-service:
    build:
      context: ../containerxdr
    ports:
      - "8081:8081"
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - NET_BIND_SERVICE
    networks:
      - bpc-net

  # ───────────── UI ───────────────
  ui:
    build:
      context: ../ui           # uses the Dockerfile we just created
    ports:
      - "8080:80"             # host:container  → browse http://localhost:8080
    depends_on:
      - sdk-service           # ensure backend is up first
    environment:
      # Runtime base URL for your frontend to call the SDK API **inside** the Docker network
      # (adapt the variable name to whatever your build expects, e.g. VITE_*/REACT_APP_*)
      VITE_API_BASE_URL: http://sdk-service:5000
      VITE_OLLAMA_URL: http://ollama-service:11434
      VITE_AICHAT_URL: http://aichat-service:5001
    restart: unless-stopped
    networks:
      - bpc-net

  # ───────────── Ollama ───────────────
  ollama-service:
    # Use the arm64 version for Apple Silicon
    image: ollama/ollama:latest
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    platform: linux/arm64  # Ensures compatibility with Apple Silicon
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - bpc-net

  # ───────────── AI Chat Service ───────────────
  aichat-service:
    build:
      context: ../aichat
    ports:
      - "5001:5001"
    depends_on:
      - ollama-service
    environment:
      - OLLAMA_URL=http://ollama-service:11434
      - OLLAMA_MODEL=tinyllama:1.1b-chat  # Use smaller model for faster startup
      - API_KEY=${API_KEY}
    restart: unless-stopped
    networks:
      - bpc-net

############################
#  SHARED NETWORK
############################
networks:
  bpc-net:
    driver: bridge

############################
#  VOLUMES
############################
volumes:
  ollama-data:
    driver: local
